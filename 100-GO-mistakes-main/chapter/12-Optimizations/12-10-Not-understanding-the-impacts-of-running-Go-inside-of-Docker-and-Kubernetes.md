## 12.10 Docker 和 Kubernetes 内部机制对 Go 程序的影响

根据 2020 Go 开发者调查，使用 Go 编写服务是最常见的用途。同时，Kubernetes 是部署这些服务最广泛使用的平台。因此，了解在 Docker 和 Kubernetes 中运行 Go 的意义以防止 CPU 节流等常见情况也很重要。

我们在并发并不总是更快中提到，`GOMAXPROCS` 变量定义了负责同时执行用户级代码的操作系统线程的限制。默认情况下，它设置为操作系统可见的逻辑 CPU 内核数。在 Docker 和 Kubernetes 的上下文中是什么意思？

假设我们的 Kubernetes 集群由 8 核节点组成。当容器部署在 Kubernetes 中时，我们可以定义 CPU 限制以确保应用程序不会消耗主机的所有资源。例如，下面的配置将 CPU 的 使用限制为 4000 millicpu（或 millicores），因此四个 CPU 内核：

```yaml
spec:
  containers:
  - name: myapp
    image: myapp
    resources:
      limits:
        cpu: 4000m
```

我们可以假设，当我们的应用程序被部署时，`GOMAXPROCS` 将基于这些限制；因此设置为 4。然而，事实并非如此；它将设置为主机上的逻辑核心数：8。那么，有什么影响？

Kubernetes 使用完全公平调度器 (CFS) 作为进程调度器。它还用于强制执行 Pod 资源的 CPU 限制。在管理 Kubernetes 集群时，管理员可以配置以下两个参数：

* `cpu.cfs_period_us`（全局设置）
* `cpu.cfs_quota_us`（每个 Pod 的设置）

前者定义了一个时期，而后者定义了一个配额。默认情况下，周期设置为 100 毫秒。同时，默认 配额值将是应用程序在 100 毫秒内可以消耗多少 CPU 时间。限制设置为四个核心，即 400 毫秒（4 * 100 毫秒）。

因此，CFS 将确保我们的应用程序在 100 毫秒的时间段内不会消耗超过 400 毫秒的 CPU 时间。

让我们想象一个场景，多个 goroutine 当前在四个不同的线程上执行，每个线程被调度在不同的核心（1、3、4 和 8）上：

![](https://img.exciting.net.cn/125.png)

在第一个 100 毫秒期间，四个线程很忙，所以我们消耗了 400 毫秒中的 400 个，因此是 100% 的配额。然后，在第二个期间，400 毫秒中的 360 毫秒等。一切都很好，因为应用程序消耗的资源少于配额。

但是，让我们记住 `GOMAXPROCS` 设置为 8。因此，在最坏的情况下，我们可以有 8 个线程，并 且每个线程都安排在不同的核心上：

![](https://img.exciting.net.cn/126.png)

每 100 毫秒的部分，配额设置为 400 毫秒。因此，如果八个线程都忙于执行 goroutine，在 50 ms 后，我们将达到 400 ms 的配额（8 * 50 ms = 400 ms）。会有什么后果？CFS 将限制 CPU 资源。因此，在另一个周期开始之前，不会再分配 CPU 资源。换句话说，我们的应用程序将暂停 50 毫秒。

例如，平均延迟为 50 毫秒的服务可能需要 150 毫秒才能完成。这可能对延迟造成 300% 的损失。

那么，解决方案是什么？首先，关注 Go issue 33803。也许，在 Go 的未来版本中，`GOMAXPROCS` 将支持 CFS。

当前的解决方案是依赖 Uber 制作的名为 `automaxprocs` 的库。我们可以通过在 `main.go` 中向 `go.uber.org/automaxprocs` 添加空白导入来使用这个库，它会自动设置 `GOMAXPROCS` 以匹配 Linux 容器 CPU 配额。在前面的示例中，`GOMAXPROCS` 将被设置为 4 而不是 8；因此，我们无法达到 CPU 被节流的状态。

总之，让我们记住，目前，Go 不支持 CFS。因此，`GOMAXPROCS` 不是基于定义的 CPU 限制，而是基于主机。因此，我们可能会达到 CPU 被节流的状态，从而导致长时间的暂停和显着的延迟增加等重大影响。在 Go 变得支持 CFS 之前，一种解决方案是依靠 `automaxprocs` 在自动定义的配额上设置 `GOMAXPROCS`。

