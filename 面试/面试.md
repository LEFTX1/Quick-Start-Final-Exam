 ![[黑白整齐简历模板 (3).pdf]]

# 技术描述部分
## Context 包
![[Pasted image 20250415194100.png]]
Context主要使用于 **调控协程之间的生命周期联动和传递元数据** 的问题。context接口有四种基本实现，分别是**空的backgroundcontext，带取消功能的cancelcontext以及带超时取消功能的timeercontext还有携带元数据value的valuecontext**,他们都可以调用四种派生方法来创建对应的子context形成一个**context树状结构。**

![[context树组成图示.svg|725]]

![[context树双向连接.svg]]

![[cancel向下传播.svg|600]]
### channel 的 csp模型是什么？
csp模型强调**通过通信来共享内存** ，具体来说就是多个并发实体Goroutine之间应该是相互独立的，它们之间的交互应该通过 Channel 来进行而不是直接访问彼此的内存数据。

### 详细说说 Context 的取消信号是如何在 Goroutine 之间传递的吗？
context内部有一个map结构来储存从他派生出来的子context，从而形成父子结构的树状结果。当一个 Context 被取消时 ，**它会遍历所有它的子 Context， 向下传递去触发它们的取消逻辑**, 也就是**关闭 (close) 那个标志着context存活信息的只读channel，这个只读 Channel**由 Done() 方法返回。读取这个channel 会因为channel无数据而阻塞，但当channel关闭后会读取到零值，也就是一个空结构体，那么使用select语句监听这个channel的case语句就会执行下去，而这一行case语句就是我们处理context取消之后的操作。

### 可以谈谈 `context` 包吗？按你的理解，它主要是用来干嘛的？"

**(候选人 - 我):** "嗯，`context` 包啊... 对，这个在 Go 里挺核心的，特别是做并发或者网络服务的时候基本绕不开。我的理解是，它主要是 Go 提供的一套标准方法，用来处理那些需要跨越多个代码部分、特别是跨 goroutine 边界的事情。比如说... 控制一个操作到底该跑多久，或者像广播一个信号说‘嘿，这个任务相关的各位，都停下吧！’，有时也用它顺便带点像请求 ID 这样的小信息。当你有好几个 goroutine 为一个初始请求工作时，它能帮你有效地管理这些复杂性。"

### `context` 具体是怎么做到像取消或者超时这种功能的？它背后的机制大概是怎样的？"

**(候选人 - 我):** "哦，这个啊，它底层很大程度上是利用 channel 来实现的。当你创建一个_可以被取消_的 context 时——比如用那个 `WithCancel` 函数，或者像 `WithTimeout` 这种带超时的——你实际上会得到一个新的 context 对象。这个对象里面有个 `Done()` 方法，它会返回一个 channel。

关键就在这儿：当这个 context 被要求取消时（不管是你手动调了它的 `cancel` 函数，还是它的时间到了），Go 就会把那个 `Done()` 返回的 channel 给关闭掉。

那么，任何拿到了这个 context 的 goroutine，就可以在自己的代码里用一个 `select` 语句块，去监听（或者说等待）这个 `ctx.Done()` channel。一旦那个 channel 被关闭，`select` 里的对应 case 立刻就能触发。这就是那个‘信号’！Goroutine 就知道：‘哦，该收尾了’，然后它就可以优雅地停止当前的工作，清理需要清理的东西，然后退出。

对了，通常在你发现 `Done()` 被关闭后，还可以调一下它的 `Err()` 方法，看看具体是_为什么_被取消的——是被人手动取消了呢，还是时间到了触发了超时。"

### 提到了 `WithCancel` 和 `WithTimeout`，那还有其他常见的方式来创建 context 吗？它们之间有啥不一样？"

**(候选人 - 我):** "嗯，除了这两个，还有几个也挺常用的。有个 `WithDeadline`，它跟 `WithTimeout` 有点像，但它不是说‘几秒后超时’，而是让你设一个具体的_时间点_，比如‘到下午五点整必须结束’。`WithTimeout` 和 `WithDeadline` 这俩都是跟时间限制有关的，而且它们也都会返回那个 `cancel` 函数，万一你想在时间到之前就提前结束任务也行。

然后还有一个挺不一样的，就是 `WithValue`。这个函数跟取消、超时没关系，它的作用纯粹是在 context 里附加一些数据，比如用户 ID 或者追踪 ID 之类的，让这些数据能一路传递下去，省得每个函数都得显式地加个参数。

哦对了，所有这些 `With` 开头的函数——`WithCancel`, `WithTimeout`, `WithDeadline`, `WithValue`——创建出来的都是子 context。有个挺方便的特性是，如果父 context 被取消或者超时了，它所有的子 context 也会自动跟着一起被取消，这个效果会级联下去。"

### 我们再说说 `WithValue`。你说它是带数据的，用它的时候有什么需要特别注意的地方吗

**(候选人 - 我):** "啊，`WithValue`... 对，用这个确实得稍微留点神。通常的建议是，用它来传递那些真正跟整个请求范围相关的信息——就是那些跨越不同处理阶段、但本身又不是核心业务逻辑必须的东西，比如我们刚说的追踪 ID，或者是一些身份认证相关的信息。

但是，有几点很重要：一般_不推荐_用它来传普通的函数参数或者依赖。如果一个函数完成工作_必须_要某个数据，最好还是明确地通过函数参数传进去。过度依赖 `WithValue` 会让代码的依赖关系变得不那么清晰，有点像是在用隐性的全局变量，维护起来可能比较麻烦。

另外一个关键点是用作 key 的东西。最佳实践是用你自己定义的、非导出的类型来做 key，而不是直接用字符串比如 `"userID"`。这样可以避免在不同的包里不小心用了相同的字符串 key 导致冲突。还有就是，你用 `Value()` 方法取值的时候，拿到的是 `interface{}` 类型，所以你得自己做类型断言，这也多了步操作，而且如果类型不对还得处理可能出现的 panic。"

### 经常看到 `context.Background()` 和 `context.TODO()`，这俩有啥区别？什么时候该用哪个呢？"

**(候选人 - 我):** "对，`Background` 和 `TODO`。它们俩基本上就是所有 context 链条的‘根’，是你可以开始构建其他 context 的起点。它们本身都是空的，永远不会被取消，也没有截止时间，也不带任何值。

主要的区别其实在于_使用的意图_和_代码的清晰度_：

- `context.Background()` 是官方推荐的、标准的用法。你应该在 `main` 函数里、或者初始化代码、测试代码这些地方用它，作为整个调用链的最顶层 context，当你不知道还能从哪儿获取父 context 时，它就是那个默认的起点。它代表一个清晰定义的、新的处理流程的开始。
- `context.TODO()` 呢，它的名字就暗示了它的意思——‘待办事项’！它表示‘我现在还不确定这里应该用哪个 context’，或者‘这块代码以后需要接入一个合适的 context，但现在还没弄好’。它就像个占位符，提醒你自己或者别人这里还有工作要做。所以，如果你在一个函数里不知道该从哪里获取 context，或者暂时无法获取时，可以用 `TODO()` 顶一下。但理想情况下，随着代码的完善，`TODO()` 最终应该被替换成从调用者传过来的、有实际意义的 context。

简单说就是：`Background` 是推荐的、明确的根，`TODO` 是个临时的、表明‘有待改进’的标记。"

###  结合你的实际项目经验来看，用 `context` 时有哪些比较好的实践方式，或者说常见的坑需要尽量避开？"

**(候选人 - 我):** "嗯……根据我的经验，确实有几点挺重要的：

- **怎么传：** 这个基本是铁律了，`context` 应该总是作为函数的**第一个参数**，而且大家通常都把它命名为 `ctx`。这算是 Go 社区的约定俗成了。
- **放哪里：** 通常，别把 `context` 塞到结构体（struct）的字段里。它应该是显式地在函数调用之间传递的，跟具体的对象实例的生命周期分开。
- **最容易踩的坑：** 可能就是光传了 `ctx`，但是忘了在自己的 goroutine 里_检查_它！尤其是在那些可能跑挺长时间的循环里，或者在等 channel 的地方，一定要在 `select` 里加上 `case <-ctx.Done():` 这个分支。不然，即使外面取消了 context，你那个 goroutine 也收不到信号，还在傻跑，这就可能导致资源泄露。
- **`WithValue` 的使用：** 就像我们前面聊的，别滥用它。只用它传递那些真正跨请求范围的、辅助性的数据。
- **别忘了 `cancel`：** 当你用 `WithCancel` 或者带超时的那几个函数创建了 context 后，它们会返回一个 `cancel` 函数。记得要调用它！最常见也最保险的做法是用 `defer cancel()`，这样能确保无论函数是正常结束还是中途 panic，这个 `cancel` 都能被调用，相关的资源能及时释放。
- **错误检查：** 当 `ctx.Done()` 被触发后，可以通过 `ctx.Err()` 来获取具体是哪种错误（比如是被取消了还是超时了），根据这个错误信息做相应的处理或者记录日志挺有用的。


## GMP 模型
![[Pasted image 20250419194134.png]]

### **Q1: 请解释一下 Go 的 GMP 模型是什么？**

**A:** GMP 是 Go 语言并发调度的核心模型。G 代表 Goroutine（轻量级并发单元），M 代表 OS 线程（执行者），P 代表逻辑处理器（调度上下文，数量由 GOMAXPROCS 控制）。GMP 模型通过 P 将大量的 G 高效地调度到少量的 M 上执行，实现了低开销的并发和对多核 CPU 的充分利用。其核心思想是用 M:N 调度（多个 G 跑在 N 个 M 上）并引入 P 作为中间层来管理 G 队列和资源，实现高效调度。

 ### **Q2: G, M, P 分别是什么？它们之间是如何协作的？**

**A:**

- **G (Goroutine):** Go 程序中的并发任务单元，栈小，用户态调度，开销低。
    
- **M (Machine):** 操作系统线程，实际执行 G 代码的载体。
    
- **P (Processor):** 逻辑处理器，M 必须获得一个 P 才能执行 G。P 维护一个本地 G 队列 (LRQ)，管理调度状态和资源。P 的数量决定了并行度。
    
- **协作：** M 需要绑定一个 P 才能工作。M 从绑定的 P 的 LRQ 获取 G 并执行。如果 LRQ 为空，M 会尝试从全局队列 (GRQ) 获取，或从其他 P 的 LRQ "窃取" (Work Stealing) G 来执行。执行 G 的过程中可能发生切换、阻塞等，触发相应的调度逻辑。
    

 ###  **Q3: Go 的调度器是如何工作的？能谈谈工作窃取机制吗？**

**A:** Go 调度器基于 GMP 模型。每个 P 有一个本地 G 队列 (LRQ)，还有一个全局 G 队列 (GRQ)。M 优先执行其绑定 P 的 LRQ 中的 G。  
**工作窃取 (Work Stealing):** 当一个 P 的 LRQ 为空，并且 GRQ 也为空时，与之绑定的 M 不会闲置。它会随机选择另一个 P，并尝试从那个 P 的 LRQ 尾部“窃取”一半的 G 到自己的 LRQ 中来执行。这有助于实现负载均衡，让所有 P (及其 M) 尽量保持忙碌，提高 CPU 利用率。

### **Q4: Goroutine 切换为什么比线程切换快得多？**

**A:** 主要原因有：

1. **用户态 vs 内核态:** Goroutine 切换完全在用户态由 Go runtime 完成，不涉及昂贵的内核态/用户态切换；线程切换由 OS 内核调度，需要模式切换。
    
2. **保存状态少:** Goroutine 切换只需保存极少的寄存器状态（主要是程序计数器 PC 和栈指针 SP）；线程切换需要保存完整的 CPU 寄存器组、内核栈信息、内存管理上下文等。
    
3. **内存管理:** Goroutine 都在同一地址空间，切换不涉及内存页表切换；线程切换（尤其跨进程）可能需要。
    
4. **栈空间:** Goroutine 初始栈小，管理更灵活；线程栈通常较大且固定。
    

### **Q5: Go 如何处理阻塞的系统调用 (Syscall)？Sysmon 的作用是什么？**

**A:**

1. **M 阻塞:** 当 G 发起阻塞 syscall，执行它的 M 会随之陷入内核阻塞。
    
2. **P 分离可能:** 为防止 P 被该阻塞 M 长时间占用而闲置，runtime 可能会将 P 从 M 解绑（P 状态置为 _Psyscall）。
    
3. **Sysmon 介入:** 后台 sysmon 线程会监控阻塞在 syscall 里的 M。如果阻塞时间过长（如超 10ms），sysmon 会认为 M 短期内不会返回。
    
4. **P Handoff:** sysmon 会强制将 P 从该 M 解绑（状态改为 _Pidle），使其能被其他空闲或新建的 M 绑定，去执行 P 上的其他 G，保证 CPU 不被浪费。
    
5. **M 返回:** 当 M 从 syscall 返回后，它需要重新找一个 P：优先找原来的 P，其次找空闲 P，再找不到则将 G 放回 GRQ，M 自己休眠。
    

### **Q6: 什么是 g0？它有什么特殊之处和作用？**

**A:** g0 是每个 P 关联的一个特殊 Goroutine，代表**调度器本身**。

- **特殊性:** 它不执行用户代码，运行在 M 的系统栈（或专用调度栈）上，栈空间固定且较大。
    
- **作用:**
    
    - 执行调度循环：寻找并切换到下一个可运行的用户 Goroutine (g)。
        
    - 处理 Goroutine 的生命周期事件：在 G 阻塞、完成、抢占时接管控制权。
        
    - 执行 runtime 任务：如执行 defer、参与 GC（栈扫描）、处理栈增长等。
        
- **重要提示:** g0 是调度执行者，但普通 Goroutine g 的切换上下文（PC/SP）是保存在 g 自己的结构体里的，不是存在 g0 里。
    

### **Q7: Go runtime 会复用 Goroutine 吗？还需要手动实现协程池吗？**

**A:**

- **会复用:** Go runtime 内建了 Goroutine 对象的复用机制。当 Goroutine 结束时，其 G 对象会被放入本地或全局的空闲列表 (freelist)，下次创建 Goroutine 时会优先从中获取复用，减少内存分配和 GC 压力。
    
- **通常不需要手动协程池:** 因为 runtime 的高效调度和内建复用，大多数场景下直接 go func() 启动 Goroutine 是最佳实践。手动创建“协程池”的主要目的**不是**为了节省 Goroutine 创建开销，而是为了**控制并发度**（例如限制同时处理任务的 worker 数量）或进行特殊的**资源管理**。
    

### Q8: 什么是线程自旋？它在 GMP 中有什么应用？**

**A:** 线程自旋是一种**忙等待**优化技术。当一个线程（在 Go 里是 M）尝试获取一个已被占用的锁或等待某个短时条件时，它会在一个紧密循环里不断检查条件是否满足，而不是立即放弃 CPU 进入睡眠。

- **在 GMP 中的应用:**
    
    - M 尝试获取 runtime 内部的锁（如调度锁、内存分配锁）时。
        
    - M 在 P 的 LRQ、GRQ 为空时，短时间自旋等待新的 G 到来或能成功窃取到 G。
        
- **目的：** 如果等待时间非常短，自旋可以避免昂贵的线程上下文切换和唤醒延迟。但如果等待时间长，会浪费 CPU。Go runtime 中的自旋通常是**自适应**的，有次数或时间限制。
    

### **Q9: GMP 模型带来了哪些优势？**

**A:**

1. **高并发:** 轻松创建和管理成千上万的 Goroutine。
    
2. **低开销:** Goroutine 创建、切换成本远低于线程。
    
3. **高效利用多核:** 通过 GOMAXPROCS 控制 P 的数量，实现真正的并行计算。
    
4. **避免阻塞:** 通过 netpoller 处理非阻塞 I/O，通过 sysmon 和 P handoff 机制处理阻塞 syscall，最大限度减少 M 因等待而被阻塞的影响。
    
5. **负载均衡:** 工作窃取机制确保 CPU 资源被充分利用。
    
6. **资源节约:** 复用 OS 线程 (M) 和 Goroutine 对象 (G)。
### **Q10: Goroutine 的栈是固定大小的吗？如果不是，它是如何管理的？**

**A:** 不是固定的。Goroutine 启动时拥有一个很小的初始栈（通常 2KB）。Go 使用**连续栈 (Contiguous Stack)** 机制来管理它。这意味着：

1. **动态增长:** 当函数调用需要的空间超过当前栈的剩余容量时，栈会自动扩容。
    
2. **连续性:** 在任何时刻，一个 Goroutine 的活动栈都存储在一块**连续**的内存区域中。
    
3. **扩容方式:** 扩容时，Go runtime 会分配一块新的、更大的连续内存（通常是旧栈两倍），将旧栈内容**完整拷贝**到新栈，**调整**旧栈内的指针指向新地址，然后**释放**旧栈内存。
    
4. **栈收缩:** GC 期间，如果发现栈长期使用率很低，也可能进行栈收缩，释放多余内存。
    

### **Q11: 什么是栈扩容？这个过程的开销如何？在什么情况下会频繁发生？**

**A:**

- **过程:** 栈扩容是 Goroutine 栈空间不足时，runtime 自动分配更大栈、拷贝旧内容、调整指针、释放旧栈的过程。它由函数入口处的栈检查（Stack Check）失败后调用的 runtime.morestack 触发。
    
- **开销:** 栈扩容**不是免费的**，其开销主要来自：
    
    - **内存拷贝 (memcpy):** 主要成本，拷贝量与旧栈大小成正比。
        
    - 内存分配、指针调整、上下文切换（到g0再回来）也有开销。
        
    - 这会导致触发扩容的函数调用产生一次**延迟**。
        
- **频繁发生场景:**
    
    - **无限/过深的递归调用:** 最常见的原因。
        
    - **在栈上分配了非常大的对象/数组。**
        
    - **极深的函数调用链。**
        
    - **特定模式下的“热分裂”遗留问题（理论上连续栈已解决，但极端深且小的调用仍可能触发多次扩容）。**
        
    - 频繁发生通常表示代码可能需要优化，应通过 profiling 确认。
        

 

### **Q18: 什么是 Cgo？在哪些业务场景下可能会用到它？**

**A:** Cgo 是 Go 语言调用 C 代码（反之亦然）的机制。在业务场景中，常见用途包括：

1. **使用 C 库的数据库驱动:** 如 go-sqlite3 (SQLite), Oracle OCI 驱动。
    
2. **集成现有 C/C++ 核心库/SDK:** 调用公司内部遗留的 C/C++ 业务逻辑、算法库，或第三方提供的只有 C/C++ 接口的 SDK。
    
3. **特定领域库:** 业务需要 GIS (GEOS/GDAL)、某些科学计算、特定协议解析等只有成熟 C/C++ 实现的库。
    
4. **安全/合规:** 需要调用 OpenSSL (如 FIPS 模式) 或与 HSM (硬件安全模块) 交互。
    

- **注意:** 使用 Cgo 会增加构建复杂性、部署依赖、带来性能开销和内存管理挑战，应优先寻找纯 Go 解决方案。

## go 内存管理

### go内存管理核心组件mheap mcentral mcache mspan
![[svg.svg|925]]


1. **`mcache` (每个工人的私人工具箱):** 每个处理 Go 程序任务的“工人”（称为 `P`，Processor）都有自己的一个小型、快速的缓存。存取自己工具箱里的东西最快，不需要和别人商量（无锁）。
2. **`mcentral` (部门共享工具柜):** 有很多个工具柜，每个柜子只存放特定 _种类/尺寸_ 的物品（Size Class）。同一个部门（所有 `P`）的工人都可以来这里取，但一次只能一个人取/放（需要加锁）。
3. **`mheap` (中央大仓库):** 这是所有部门共享的、最大的仓库。管理着大量的、成块的“空地”（内存页 Pages）。从这里调拨资源比较慢，需要和仓库主管协调（需要加锁）。
4. **OS (外部供应商):** 如果中央大仓库也没货了，就只能向操作系统这个“外部供应商”订购更大块的“土地”（通过 `mmap` 等系统调用）。这是最慢的方式。

---

### 3.3 小对象分配 (<= 32KB) —— 拿小零件

想象你要拿一个小螺丝（一个小于等于 32KB 的内存块）。

**流程图（文字模拟）：**

```
你要拿小螺丝 (请求内存)
  │
  ▼
1. 确定螺丝规格 (计算 Size Class: 把你需要的大小，归到最近的标准规格里)
  │
  ▼
2. 查看【我的工具箱 mcache】里，对应规格的盒子(mspan)还有没有?
  │   │
  │   ├─> 有空位? (allocation bitmap) -> 太好了! 直接拿走 (标记已用, 返回地址) 【最快! 无锁】
  │   │
  │   └─> 没有空位 / 没有这种规格的盒子?
  │          │
  │          ▼
  │       3. 去【部门工具柜 mcentral】(对应规格的) 申请一整盒新的螺丝 (mspan)
  │          │   │
  │          │   ├─> 工具柜里有现成的盒子? (加锁访问) -> 拿到盒子，放进【我的工具箱 mcache】-> 回到第 2 步
  │          │   │
  │          │   └─> 工具柜里也没有了?
  │          │          │
  │          │          ▼
  │          │       4. 工具柜管理员去【中央大仓库 mheap】申请一块空地 (Pages) 来装新的螺丝盒
  │          │          │   │
  │          │          │   ├─> 仓库里有合适的空地? (加锁访问) -> 拿到空地，做成新的螺丝盒(mspan)，交给【部门工具柜 mcentral】-> 【部门工具柜】再给我一盒 -> 回到第 2 步
  │          │          │   │
  │          │          │   └─> 仓库里也没有足够大的连续空地?
  │          │          │          │
  │          │          │          ▼
  │          │          │       5. 仓库管理员向【外部供应商 OS】订购一大块新土地 (mmap) 【最慢!】
  │          │          │          │ -> 新土地入库【中央大仓库 mheap】-> 回到第 4 步
  │          │          │
  │          ▼          ▼
  │       (拿到螺丝后)
  ▼       6. 这颗螺丝需要擦干净吗? (needzero 标志 / 明确要求) -> 如果需要，擦干净 (内存清零)
  │
  ▼
递给你干净的螺丝 (返回内存指针)
```

**简单说：**

1. **先看自己手边 (`mcache`) 有没有？** 这是最快的方式，不用跟任何人打交道。
2. **手边没有，去部门仓库 (`mcentral`) 领一整盒。** 需要排队（加锁），但领回来后又能快速用了。
3. **部门仓库也没有，部门管理员去中央仓库 (`mheap`) 申请原料。** 更慢，也要排队（加锁）。
4. **中央仓库也没原料了，只能向外面 (`OS`) 订购。** 这是最慢的兜底方案。
5. **最后，按需把拿到的东西擦干净（清零）。**

这种层层递进的方式，确保了最高频的小对象分配尽可能发生在最快的 `mcache` 层面，大大减少了需要加锁和访问慢速资源的次数。

---

### 3.4 大对象分配 (> 32KB) —— 搬大机器

想象你要搬一台大机器（一个大于 32KB 的内存块）。这种大家伙，你的小工具箱和部门工具柜都放不下。

**流程图（文字模拟）：**

```
你要搬大机器 (请求内存 > 32KB)
  │
  ▼
1. 计算需要多大的场地 (向上取整到 8KB 的倍数，即多少个 Page)
  │
  ▼
2. 直接去【中央大仓库 mheap】申请这么大的连续空地
  │   │
  │   ├─> 仓库里有足够大的连续空地? (加锁访问) -> 分配空地，用栅栏围起来(创建 mspan 管理) -> 跳到第 4 步
  │   │
  │   └─> 仓库里没有这么大的连续空地?
  │          │
  │          ▼
  │       3. 仓库管理员向【外部供应商 OS】订购所需大小的新土地 (mmap) 【慢!】
  │          │ -> 新土地入库【中央大仓库 mheap】-> 回到第 2 步 (重新尝试在仓库分配)
  │
  ▼
3. 把分配到的场地彻底打扫干净 (内存清零) 【大对象总是清零】
  │
  ▼
把场地的入口指给你 (返回内存指针，即 mspan 起始地址)
```

**简单说：**

1. **直接跳过** `mcache` 和 `mcentral`，因为它们处理不了这么大的东西。
2. **直接去中央大仓库 (`mheap`)** 申请一块足够大的连续空间。需要排队（加锁）。
3. **仓库空间不够，就向外面 (`OS`) 订购。**
4. **拿到空间后，一定打扫干净（清零）** 再交给你用。

这个流程更直接，因为它知道小缓存和共享柜处理不了大件，索性直接去能处理的地方。

---

### 栈管理、栈扩容、内存分配细节

### **Q10: Goroutine 的栈是固定大小的吗？如果不是，它是如何管理的？**

**A:** 不是固定的。Goroutine 启动时拥有一个很小的初始栈（通常 2KB）。Go 使用**连续栈 (Contiguous Stack)** 机制来管理它。这意味着：

1. **动态增长:** 当函数调用需要的空间超过当前栈的剩余容量时，栈会自动扩容。
    
2. **连续性:** 在任何时刻，一个 Goroutine 的活动栈都存储在一块**连续**的内存区域中。
    
3. **扩容方式:** 扩容时，Go runtime 会分配一块新的、更大的连续内存（通常是旧栈两倍），将旧栈内容**完整拷贝**到新栈，**调整**旧栈内的指针指向新地址，然后**释放**旧栈内存。
    
4. **栈收缩:** GC 期间，如果发现栈长期使用率很低，也可能进行栈收缩，释放多余内存。
    

### **Q11: 什么是栈扩容？这个过程的开销如何？在什么情况下会频繁发生？**

**A:**

- **过程:** 栈扩容是 Goroutine 栈空间不足时，runtime 自动分配更大栈、拷贝旧内容、调整指针、释放旧栈的过程。它由函数入口处的栈检查（Stack Check）失败后调用的 runtime.morestack 触发。
    
- **开销:** 栈扩容**不是免费的**，其开销主要来自：
    
    - **内存拷贝 (memcpy):** 主要成本，拷贝量与旧栈大小成正比。
        
    - 内存分配、指针调整、上下文切换（到g0再回来）也有开销。
        
    - 这会导致触发扩容的函数调用产生一次**延迟**。
        
- **频繁发生场景:**
    
    - **无限/过深的递归调用:** 最常见的原因。
        
    - **在栈上分配了非常大的对象/数组。**
        
    - **极深的函数调用链。**
        
    - **特定模式下的“热分裂”遗留问题（理论上连续栈已解决，但极端深且小的调用仍可能触发多次扩容）。**
        
    - 频繁发生通常表示代码可能需要优化，应通过 profiling 确认


### Go 对小于 16 字节且不包含指针的对象有什么特殊处理吗？
1. **垃圾回收 (GC)**:
    
    - **无指针对象扫描**: 这是最重要的优化。如果 Go 的编译器和运行时确定一个类型不包含任何指针（无论是直接的还是嵌套的），它会在分配该类型的对象时，在对应的内存元信息（例如 gcbits 位图）中标记该内存区域为“无指针”。
        
    - **跳过扫描**: 在 GC 的标记（Mark）阶段，当扫描器遇到被标记为“无指针”的内存块时，它会完全跳过扫描该内存块的内容。扫描器只需要知道这个对象的大小，然后直接跳到下一个对象。这大大减少了 GC 的扫描工作量，特别是当存在大量此类小对象时。对于包含指针的对象，GC 必须仔细检查其内容以查找并跟踪其他活动对象的引用。
        
    - **大小无关**: 这个“无指针”优化本身与对象大小（是否小于 16 字节）没有直接关系，但它对所有不含指针的对象都适用。然而，小对象通常数量更多，因此这种优化的累积效应可能更显著。
        
2. **内存分配**:
    
    - **微小对象分配器 (Tiny Allocator)**: Go 的内存分配器对非常小的对象（通常是 <= 16 字节且无指针的对象）有特殊的优化。这些对象可能会被分配到一个称为“tiny block”的特殊区域，或者使用特定的 size class 进行管理。
        
    - **Size Classes**: Go 的分配器使用预定义的 size classes 来管理不同大小的内存块。小于 16 字节的对象会落入最小的几个 size class 中。分配器会为这些 size class 维护专门的 mspan（内存管理单元），并通常从线程本地缓存 (mcache) 中快速分配，减少了锁竞争和分配开销。
        
    - **无指针优化**: 结合 GC 的无指针标记，分配器可以更有效地管理这些小块内存，因为知道它们不需要被 GC 扫描。
        
3. **栈分配 (Escape Analysis)**:
    
    - **更易于栈分配**: Go 的编译器会进行逃逸分析（Escape Analysis），尝试将对象的分配从堆（Heap）移到栈（Stack）上。栈分配非常快，并且不需要 GC 来管理。
        
    - **小尺寸优势**: 小对象（如小于 16 字节）由于复制成本低，更有可能被编译器判断为适合在栈上分配（如果它们的生命周期没有逃逸出当前函数）。
        
    - **无指针简化**: 虽然不是决定性因素，但不包含指针的简单结构体使得逃逸分析更容易进行。
        
4. **值传递和复制**:
    
    - **低成本复制**: 对于这么小的对象，在函数调用时按值传递（复制整个对象）的开销非常低。这通常比传递指针（需要解引用，可能导致缓存未命中）然后访问堆上数据的开销还要小，并且避免了潜在的堆分配。


### 你对 Go 的内存管理机制了解多少？
好的，面试官。嗯... 我的理解是，Go 语言的内存管理最大的特点就是它**自带了垃圾回收（GC）机制**，开发者基本上不需要手动去申请和释放内存，这一点跟 C/C++ 很不一样，可以大大减少内存泄漏的风险，也减轻了开发者的心智负担。

Go 的内存管理主要是围绕 **自动内存分配** 和 **自动垃圾回收** 这两个核心来的。

- **内存分配方面**，Go 为了提高效率，自己管理了一个**内存池**。它会向操作系统申请一大块内存，然后自己切分成不同大小的 `span` (内存块) 来管理。对于不同大小的对象，它有不同的分配策略。比如小对象，它会倾向于从一个叫做 `mcache` 的 per-P（处理器）的本地缓存里分配，这样可以减少锁的竞争，速度很快。如果没有或者对象比较大，可能就会去 `mcentral` (中心缓存) 或者直接去 `mheap` (堆) 上分配了。
- **垃圾回收方面**，Go 现在主要使用的是**并发的三色标记清除法** (Concurrent Mark and Sweep)。这个 GC 最大的优点就是它大部分工作是**和用户 Goroutine 并发执行**的，只有很短的 STW (Stop The World) 时间，所以对程序造成的卡顿影响很小，这也是 Go 适合做高并发服务的一个重要原因。

### 你刚才提到了“并发三色标记清除法”，能稍微展开讲讲这个 GC 的过程吗？
**候选人:** 当然可以。简单来说，三色标记法就是把内存中的对象分成三类：

1. **白色对象**：代表可能是垃圾，待检查的对象。初始时所有对象都是白色的。
2. **灰色对象**：代表自身是存活的，但是它引用的对象还没检查完。GC 会从根对象（比如全局变量、执行栈上的变量）开始，把它们标记为灰色。
3. **黑色对象**：代表自身是存活的，并且它引用的所有对象也都检查过了（或者已经被标记为灰色了）。

GC 的主要流程就是：

1. **开始标记 (Mark Setup)**：会有一个非常短暂的 STW，主要是做一些准备工作，比如开启写屏障 (Write Barrier)。写屏障很重要，它就像一个监控，能在标记过程中，如果用户 Goroutine 修改了对象间的引用关系（比如一个黑色对象指向了一个白色对象），它能保证这个白色对象不会被错误地回收掉，通常是把它重新标记为灰色。
2. **并发标记 (Marking)**：这是 GC 最耗时的阶段，但它是和用户 Goroutine 并发执行的。GC 会不断地从灰色对象集合里拿出对象，把它引用的所有白色对象都标记为灰色，然后把自己标记为黑色。这个过程一直持续，直到没有灰色对象为止。
3. **标记结束 (Mark Termination)**：也会有一个 STW，时间也比较短。主要是完成标记工作，关闭写屏障。
4. **并发清扫 (Sweeping)**：这个阶段也是并发的。GC 会遍历所有的内存块 (`mspan`)，把所有白色对象（也就是垃圾）占用的内存回收掉，方便后续分配。

哦对了，这个过程中，写屏障 (Write Barrier) 和辅助 GC (Mutator Assist) 是保证并发正确性和效率的关键技术。写屏障保证不错杀，辅助 GC 会让分配内存的用户 Goroutine 帮忙做一些标记工作，防止 GC 进度跟不上分配速度。

### 除了 GC 回收，在内存分配这块，Go 是怎么区分对待小对象和大对象的呢？它们分配的路径有什么不同？
**候选人:** 嗯，这个处理方式是不一样的。Go 内部会对要分配的内存大小做一个判断。

- **对于小对象**（一般是小于等于 32KB 的），Go 会有一套精细化的管理策略。它会把内存页（通常是 8KB）切割成很多个固定大小的小块（`object`），然后用 `mspan` 来管理这些同样大小的小块。分配的时候，会先尝试从当前 Goroutine 所在的 P 的本地缓存 `mcache` 里找对应的 `mspan`，这里分配几乎没有锁，非常快。如果 `mcache` 里没有合适的 `mspan`，就会去 `mcentral` 里加锁获取一个，`mcentral` 是所有 P 共享的，它会管理着各种大小规格的 `mspan` 列表。如果 `mcentral` 也没有，才会向 `mheap` 申请内存页，切割成 `mspan` 再分配。
- **对于大对象**（大于 32KB 的），Go 就不会走 `mcache` 和 `mcentral` 这套复杂的缓存机制了，它会直接从 `mheap` 上分配足够数量的连续内存页。因为大对象分配的频率相对较低，而且每次分配的内存量大，直接走 `mheap` 更简单高效。

总的来说，就是用缓存和分级策略来优化小对象的分配速度和内存碎片问题，大对象则直接向堆申请。

### 我们平时写的变量，比如函数里的局部变量，Go 是怎么决定把它放在栈 (stack) 上还是堆 (heap) 上呢？是开发者指定的吗？
这个不是开发者显式指定的，Go 编译器会自动进行**逃逸分析 (Escape Analysis)** 来决定。

简单来说，编译期，编译器会分析一个变量的作用域和生命周期。

- 如果一个变量只在函数内部使用，它的生命周期明确，并且函数返回后就不再需要了，那么它通常会被分配在**栈**上。栈内存分配和回收非常快，只需要移动栈指针就行，开销很小。
    
- 但是，如果编译器发现这个变量的生命周期可能会超过这个函数本身，比如：
    
    - 这个变量的**指针被函数返回**了。
    - 这个变量被**闭包引用**了，并且这个闭包在函数返回后还可能被调用。
    - 这个变量被**发送到了 channel** 里（因为不知道接收方什么时候处理）。
    - 变量太大，超过了栈的限制（虽然比较少见）。
    - 或者被 `slice` 或 `map` 的 `value` 间接引用，并且 `slice` 或 `map` 本身逃逸了。
    
    只要出现类似这些情况，编译器就认为这个变量**“逃逸”**了，必须把它分配在**堆**上，这样即使函数返回了，它指向的内存也不会被立刻回收，可以通过 GC 来管理它的生命周期。


## channel
![[Pasted image 20250419214222.png]]
![[Pasted image 20250419215703.png]]![[Pasted image 20250419215707.png]]

---


### 介绍一下 Golang 中的 Channel 是什么

**候选人（我）**：面试官你好！Golang 的 Channel 是一种用于在不同 Goroutine 之间进行通信和同步的管道（Pipe）。你可以把它想象成一个类型安全的队列，数据可以从一端被发送进去（`<-` 操作符用于发送），然后从另一端被接收出来（`<-` 操作符也用于接收）。

Channel 的主要目的是解决并发编程中的两个核心问题：

1. **Goroutine 间的通信**：让不同的 Goroutine 可以安全地交换数据，避免了传统共享内存+锁（Mutex）模式下可能出现的复杂性和潜在的数据竞争（Race Condition）问题。Go 提倡 "不要通过共享内存来通信，而要通过通信来共享内存"。
2. **Goroutine 间的同步**：Channel 的发送和接收操作本身具有阻塞性（对于某些类型的 Channel），这可以被用来协调 Goroutine 的执行顺序，比如等待一个 Goroutine 完成任务后再继续执行。

### 讲讲 unbuffered channel 和 buffered channel 的区别

**候选人**：当然。Channel 主要分为两种：

1. **Unbuffered Channel (无缓冲通道)**：
    
    - 创建方式：`make(chan T)`，其中 T 是通道传输的数据类型，容量为 0。
    - **特点**：发送操作 (`ch <- data`) 会阻塞，直到有另一个 Goroutine 准备好从该 Channel 接收数据 (`<- ch`)。同样，接收操作也会阻塞，直到有另一个 Goroutine 向该 Channel 发送数据。这种方式也被称为同步通道，因为它强制发送和接收操作同步发生。
    - **使用场景**：
        - 需要强同步保证的场景，确保发送方知道接收方已经准备好接收，或者接收方知道发送方已经发送了数据。
        - 作为信号量使用，例如通知任务完成。发送一个值，接收方接收到即表示信号到达。
2. **Buffered Channel (有缓冲通道)**：
    
    - 创建方式：`make(chan T, capacity)`，其中 `capacity > 0`。
    - **特点**：发送操作只有在缓冲区满时才会阻塞。接收操作只有在缓冲区空时才会阻塞。只要缓冲区未满，发送操作就可以立即完成（异步）；只要缓冲区不空，接收操作就可以立即完成。
    - **使用场景**：
        - 解耦生产者和消费者：允许生产者和消费者以不同的速率工作，缓冲区可以作为临时的存储。
        - 提高吞吐量：在某些情况下，允许一定程度的异步可以减少 Goroutine 阻塞等待的时间。
        - 实现类似信号量或限制并发数的模式：例如，创建一个容量为 N 的 buffered channel，工作 Goroutine 在开始工作前向 channel 发送一个值（获取令牌），工作结束后再接收一个值（释放令牌）。

### 那么，向一个已经关闭的 Channel 发送数据会发生什么？从一个已经关闭的 Channel 接收数据呢？为什么需要关闭 Channel？

**候选人**：操作已关闭的 Channel 会有以下行为：

1. **向已关闭的 Channel 发送数据**：会导致程序 panic。这是因为关闭 Channel 意味着不会再有新的数据进入，继续发送违反了这个约定。
2. **从已关闭的 Channel 接收数据**：
    - 如果 Channel 的缓冲区中还有数据，接收操作会成功，依次返回缓冲区中的值。
    - 如果 Channel 的缓冲区已经为空，接收操作会立即返回，得到的是该 Channel 元素类型的零值（例如，`int` 类型是 `0`，`string` 类型是 `""`，指针是 `nil`）。
    - 为了区分接收到的是正常值还是因为 Channel 关闭而得到的零值，可以使用多重返回值的方式接收：`value, ok := <- ch`。如果 `ok` 为 `true`，表示成功接收到了一个有效值 `value`；如果 `ok` 为 `false`，表示 Channel 已经被关闭且缓冲区为空，此时 `value` 是零值。

为什么要关闭 Channel：

关闭 Channel 主要用于通知接收方：不会再有新的数据发送到这个 Channel 了。这对于接收方使用 range 循环来处理 Channel 数据尤为重要。如果没有关闭 Channel，range 循环会一直阻塞等待新的数据，导致死锁。当 Channel 被关闭后，range 循环会在读取完所有缓冲数据后自动结束。

### **面试官**：那对一个 nil channel 进行读写操作会发生什么？

**候选人**：对 `nil` channel（即未初始化的 channel 或被赋值为 `nil` 的 channel）进行操作会导致：

- **向 `nil` channel 发送数据**：会永久阻塞当前 Goroutine。
- **从 `nil` channel 接收数据**：会永久阻塞当前 Goroutine。
- **关闭 `nil` channel**：会导致程序 panic。

`nil` channel 在 `select` 语句中有一个特殊的用途：可以用来禁用 `select` 中的某个 `case` 分支。如果 `select` 中的某个 case 涉及的 channel 是 `nil`，那么这个 case 将永远不会被选中。

**面试官**：提到 `select`，你能解释一下 `select` 语句的作用以及它是如何处理多个 Channel 操作的吗？

**候选人**：`select` 语句是 Go 语言中处理异步 I/O 或多路 Channel 通信的核心机制。它类似于 `switch` 语句，但其 `case` 后面跟的是 Channel 的发送或接收操作。

`select` 的主要作用是：**同时监听多个 Channel 操作，并在其中一个可以进行（非阻塞）时执行相应的 case 代码块。**

其行为特点如下：

1. **监听**：`select` 会监听所有 `case` 中涉及的 Channel 操作（发送或接收）。
2. **选择**：
    - 如果**只有一个** case 的 Channel 操作可以立即进行（即不会阻塞），则执行该 case。
    - 如果**有多个** case 的 Channel 操作都可以立即进行，`select` 会**随机选择**其中一个执行。这是为了防止饥饿，保证公平性。
    - 如果**所有** case 的 Channel 操作都需要阻塞，`select` 的行为取决于是否有 `default` 子句：
        - **有 `default` 子句**：执行 `default` 子句，`select` 语句不会阻塞。这常用于实现非阻塞的 Channel 操作检查。
        - **没有 `default` 子句**：`select` 语句会阻塞，直到其中一个 Channel 操作变得可以进行为止。
3. **`nil` channel 的处理**：如刚才提到的，如果 `select` 的某个 case 涉及的操作是针对 `nil` channel 的，那么这个 case 将永远不会被选中。

`select` 广泛应用于：超时控制、多任务协调、退出信号处理等场景。

### 能举例说明一下可能导致死锁（Deadlock）的情况吗？

**候选人**：使用 Channel 时确实有一些常见的陷阱和需要注意的地方：

1. **死锁 (Deadlock)**：这是最常见的问题。当程序中所有的 Goroutine 都被阻塞，无法继续执行时，就会发生死锁。常见导致死锁的情况包括：
    
    - **主 Goroutine 等待子 Goroutine，但子 Goroutine 却在等待主 Goroutine 或其他已阻塞的 Goroutine**。
    - **向 unbuffered channel 发送数据，但没有接收者**：`ch := make(chan int); ch <- 1` (在单个 Goroutine 中执行，会死锁)。
    - **从 unbuffered channel 接收数据，但没有发送者**：`ch := make(chan int); <- ch` (在单个 Goroutine 中执行，会死锁)。
    - **向已满的 buffered channel 发送数据**。
    - **从已空的 buffered channel 接收数据**。
    - **循环等待**：Goroutine A 等待 Goroutine B，Goroutine B 等待 Goroutine A。
    - **`range` 一个未关闭的 Channel**：如果所有发送者都已退出，但 Channel 未关闭，`range` 循环会永久阻塞等待。
2. **Panic**：
    
    - 向已关闭的 Channel 发送数据。
    - 关闭一个已经关闭的 Channel。
    - 关闭一个 `nil` channel。
3. **资源泄露 (Goroutine Leak)**：如果 Goroutine 因为等待 Channel 操作（如从 Channel 接收或向 Channel 发送）而被永久阻塞，并且永远没有机会解除阻塞（例如，对应的发送者或接收者已经退出，或者 Channel 永远不会被关闭），那么这个 Goroutine 就泄露了，它占用的资源无法释放。
    
4. **误用 `nil` channel**：忘记初始化 Channel（使其为 `nil`）然后进行读写，导致永久阻塞。
    

**避免策略**：

- 仔细设计 Goroutine 间的通信模式，确保发送和接收操作能够匹配。
- 对于需要结束的 Channel，发送方负责关闭它，并且只关闭一次。
- 在可能阻塞的地方使用 `select` 配合 `default` 或超时机制。
- 使用 `sync.WaitGroup` 等待一组 Goroutine 完成，而不是仅仅依赖 Channel 通信来判断。
- 注意 `range` Channel 的退出条件，确保 Channel 会被关闭。

**面试官**：非常棒！你对 Channel 的理解很深入，也考虑到了很多实践中的细节。今天的面试就到这里，感谢你的参与。

**候选人**：谢谢面试官！我也很高兴能和您交流。

---


# 计算机网络

## http 
### HTTP 常见的状态码有哪些？

2xx 开头表示成功，比如 200 OK。
3xx 开头表示重定向，比如 301 永久搬家，304 东西没变用缓存。
4xx 开头表示客户端请求有问题，比如 404 Not Found 找不到，403 Forbidden 不让看。
5xx 开头表示服务器那边出问题了，比如 500 Internal Server Error 服务器内部出错。
HTTP 常见字段有哪些？

就是请求和响应头里带的一些说明信息，像 Host 说明访问哪个网站，Content-Type 说明内容格式，Cookie 用来带用户登录状态之类的信息，Cache-Control 指示怎么缓存。
### GET 和 POST 有什么区别？

通常 GET 用来拿数据，参数放 URL 里；POST 用来交数据，参数通常放在请求体（body）里。GET 一般不改服务器数据，POST 会改。GET 请求能缓存，POST 一般不行。
### GET 和 POST 方法都是安全和幂等的吗？

按规矩说，GET 应该是安全（不改变服务器数据）且幂等（请求一次和多次效果一样）的。POST 通常两者都不是。但实际开发中不一定严格遵守。
### HTTP 缓存有哪些实现方式？

主要就两种：强制缓存和协商缓存。
### 什么是强制缓存？

就是浏览器看自己存的这份缓存还没到期（根据 Cache-Control 或 Expires），就直接用了，不跟服务器打招呼。
### 什么是协商缓存？

就是强制缓存过期了，浏览器带点儿上次缓存的信息（像 ETag 版本号或 Last-Modified 时间）去问服务器：“我这份旧的还能用吗？” 服务器对比一下，没变就回个 304，让用旧的；变了就回 200 带上新的。
### HTTP/1.1 的优点有哪些？

主要优点是简单，基于文本容易懂；而且通用，用得非常广泛。
### HTTP/1.1 的缺点有哪些？

缺点主要是：性能上有队头阻塞 (Head-of-Line Blocking) 问题，意思是在同一个 TCP 连接上，响应必须按请求顺序返回，所以前一个响应处理慢或异常了就会阻塞后续所有响应的发送和接收，导致连接效率低下；此外还有明文传输不安全；请求头部信息冗余；协议无状态管理起来也比较麻烦。
### HTTP/1.1 的性能如何？

性能比 1.0 强，主要是靠长连接 (Keep-Alive)，这个机制指的是在一个 TCP 连接建立后可以传输多个 HTTP 请求和响应，避免了像 HTTP/1.0 那样每个请求都重新建立 TCP 连接（三次握手）的开销。但它的主要性能瓶颈还是队头阻塞（前面已解释），这限制了单个连接上的并发处理能力，所以在复杂页面场景下性能一般。
### HTTP 与 HTTPS 有哪些区别？

HTTPS 就是给 HTTP 加了层 SSL/TLS 安全协议，传输内容是加密的，更安全。用的端口不一样（HTTP 80, HTTPS 443），HTTPS 服务器需要证书。
### HTTPS 解决了 HTTP 的哪些问题？

解决了 HTTP 明文传输被窃听、被篡改、服务器被假冒这三大安全风险。
HTTPS 是如何建立连接的？其间交互了什么？

比 HTTP 多一步 TLS 握手过程。逻辑上是：双方先确认对方身份（主要是客户端验证服务器证书），然后协商确定本次通信使用的加密套件，最后安全地生成和交换用于加密应用数据的会话密钥。
### HTTPS 的应用数据是如何保证完整性的？

通过消息认证码 (MAC) 机制。发送数据时，会根据数据内容和双方共享的会话密钥算出一个 MAC 值，附加在加密数据旁。接收方解密后，用同样方法计算 MAC，与收到的 MAC 比对，一致则说明数据未被篡改。
### HTTPS 一定安全可靠吗？

协议本身设计是安全的。但实际安全性依赖于正确的证书验证（比如用户不能忽略浏览器警告）和客户端环境的安全（比如操作系统信任的根证书列表未被污染）。否则，仍可能遭受中间人攻击。
### HTTP/1.1 相比 HTTP/1.0 提高了什么性能？

最主要的提升是默认启用了长连接 (Keep-Alive)，也就是可以用一个 TCP 连接处理多个 HTTP 请求，省去了频繁建立和断开 TCP 连接（三次握手）的开销。
### HTTP/2 做了什么优化？

核心是多路复用，允许在一个 TCP 连接上并行、交错地处理多个请求/响应流，解决了 HTTP/1.1 应用层的队头阻塞问题。还有头部压缩 (HPACK)、二进制传输、服务器推送等也提升了效率。
### HTTP/3 做了哪些优化？

最大改变是底层换用了基于 UDP 的 QUIC 协议。因为 HTTP/2 虽解决了应用层阻塞，但 TCP 协议本身为了保证数据按序到达，在网络丢包时会暂停所有流的数据交付，这叫 TCP 层的队头阻塞。QUIC 在 UDP 之上为每个流独立管理可靠性，一个流的丢包不影响其他流，因此彻底解决了队头阻塞问题。而且 QUIC 连接建立更快，还能支持连接迁移（切换网络时保持连接）

### RSA vs ECDHE 握手区别（精简版）：
- **RSA 交换：**
    
    - 客户端**生成**一个秘密（预主密钥）。
    - 用服务器证书里的**长期公钥加密**这个秘密，发给服务器。
    - 服务器用自己的**长期私钥解密**得到秘密。
    - **缺点：** 没有**前向安全性**（服务器私钥丢了，历史通信可能被解密）。
- **ECDHE 交换：**
    
    - 客户端和服务器都**临时生成**密钥对。
    - 双方交换**临时公钥**（服务器会用长期私钥**签名**自己的临时公钥信息，证明身份）。
    - 双方**各自独立**用自己的临时私钥和对方的临时公钥，通过算法**算出**同一个秘密（预主密钥），这个秘密**不直接在网络上传输**。
    - **优点：** 有**前向安全性**（服务器私钥丢了，不影响历史通信安全）。
## tcp
好的，这是使用 Markdown 格式化的面试对话：

---

### 为什么我们还需要 `TCP`？它解决了什么核心问题？

**面试者:** 嗯，是的，`IP` 层确实是“尽力而为”，它不保证数据包一定能到，也不保证按顺序到，甚至可能损坏。`TCP` 主要就是建立在 `IP` 之上，来解决这些可靠性的问题。它的核心目标就是提供一个可靠的、面向连接的、基于字节流的传输服务。简单说，就是确保应用程序发送的数据，能完整、有序、没有差错地到达对方应用程序，就像在两者之间建立了一个可靠的管道一样，尽管底下的网络环境可能很复杂、不可靠。它通过`序列号`解决了乱序，通过`确认`和`重传`解决了丢包，还有`校验和`来保证数据完整性。

### `TCP` 设计了`三次握手`。为什么是三次，而不是两次或者更简单的方式呢？这里面有什么关键的考量？

**面试者:** 对，`三次握手`主要是为了确保双方都能确认对方的接收和发送能力都正常，并且能同步初始`序列号`。最关键的一点，其实是为了防止“失效的连接请求报文”突然又传到服务器，导致服务器错误地建立连接。

想象一下，如果客户端发的第一个连接请求（`SYN`）在网络里滞留了，客户端超时重发了一个新的 `SYN`，这次成功建立了连接，然后数据传输完了，连接也关闭了。这时候，那个迷路的旧 `SYN` 突然到达了服务器。如果是两次握手，服务器收到这个旧 `SYN`，会以为是新的请求，就分配资源，然后向客户端发送确认，然后就等着... 但客户端其实根本没想建立新连接，这就白白浪费了服务器资源。

`三次握手`就能避免这个问题。服务器收到旧 `SYN`，回复 `SYN+ACK`，客户端收到后发现这个确认号对不上（不是它期望的），就会发送一个 `RST` 报文，告诉服务器“出错了”，服务器就知道这是个无效请求，就不会建立连接。所以，这三次交互能有效地防止这种历史连接请求造成的问题，同时也确保了双方初始`序列号`的可靠同步。

### `四次挥手`，尤其是在主动关闭连接的一方 `TIME_WAIT` 状态为什么是必要的？它又可能带来什么问题？

**面试者:** 是的，`TIME_WAIT` 是主动关闭方在发送完最后一个 `ACK` 后进入的状态。它主要是出于两个原因：

1.  **确保网络中残余的数据包（尤其是可能重传的 `FIN`）能彻底消失。** 因为网络是复杂的，报文可能延迟，如果不等待一段时间，新建立的、使用相同`四元组`（源 IP、源端口、目的 IP、目的端口）的连接可能会收到上一次连接残留的数据包，造成混乱。等待 `2MSL` (报文最大生存时间的两倍) 基本能保证双向的所有旧报文都消失了。
2.  **确保被动关闭方能可靠地收到最后的 `ACK`。** 如果主动方发的最后一个 `ACK` 丢失了，被动关闭方会收不到确认，就会超时重传它的 `FIN`。主动方必须还在 `TIME_WAIT` 状态才能接收到这个重传的 `FIN`，然后重新发送 `ACK`，让对方能正常关闭。

至于问题嘛，最常见的就是 `TIME_WAIT` 状态会占用本地端口。如果一个服务（特别是作为客户端去连接其他服务时）在短时间内需要建立大量连接并主动关闭，可能会因为大量连接处于 `TIME_WAIT` 而耗尽可用的源端口号，导致无法建立新的连接。

**面试官:** 确实，端口占用是个问题。那既然 `TCP` 这么可靠，为什么还会有 `UDP` 的存在呢？在什么场景下我们会选择 `UDP` 而不是 `TCP`？

**面试者:** 嗯，这是个很好的权衡问题。`TCP` 的可靠性是有代价的，需要建立连接、发送确认、处理重传、进行`流量控制`和`拥塞控制`，这些都会带来额外的开销和一定的延迟。`UDP` 呢，它就很简单，基本上就是在 `IP` 的基础上加了个端口号，它不保证可靠性，没有连接状态，开销非常小，传输速度快。

所以，在那些对实时性要求很高，或者能容忍少量丢包的场景下，`UDP` 就很有优势。比如说：

*   **在线游戏、视频直播、语音通话：** 这些场景下，丢失一两个数据包可能只是造成短暂的卡顿或花屏，影响通常可以接受，但如果用 `TCP` 那样为了保证可靠性而引入延迟和重传，体验可能会更差。
*   **`DNS` 查询：** 通常数据量很小，一次请求响应就结束了，用 `UDP` 的效率就很高。

所以，选择 `TCP` 还是 `UDP`，主要看应用场景对可靠性、实时性和效率的需求如何权衡。

**面试官:** 除了 `TIME_WAIT`，有时我们也会在服务器上看到大量的 `CLOSE_WAIT` 状态。这通常暗示了什么问题？跟 `TIME_WAIT` 比，它的成因有什么不同？

**面试者:** `CLOSE_WAIT` 状态通常意味着服务器这边（被动关闭方）收到了客户端发来的 `FIN`，也回复了 `ACK`，表示“我知道你要关了”，但是服务器端的应用程序自己还没有调用关闭连接的操作（比如调用 `close`）。所以，`TCP` 连接在内核层面并没有完全关闭。

这跟 `TIME_WAIT` 完全不同，`TIME_WAIT` 是主动关闭方在完成所有事情后等待的状态。而大量的 `CLOSE_WAIT` 通常是一个比较明确的信号，表明服务器应用程序本身可能存在问题。比如：

*   程序逻辑有 bug，忘记关闭不再使用的连接；
*   处理请求的某个环节阻塞了，导致无法执行到关闭连接的代码；
*   资源泄漏，比如文件描述符耗尽，导致无法正常关闭 `socket`。

总之，看到很多 `CLOSE_WAIT`，一般需要去检查应用程序的代码逻辑。

**面试官:** 这听起来像是应用层的问题。稍微关联一下编程，我们知道服务器接受连接需要调用 `listen` 和 `accept`。这个 `accept` 函数，它是在`三次握手`的哪个阶段之后才会返回呢？它跟连接状态有什么关系？

**面试者:** `accept` 函数是在 `TCP` 的`三次握手` **完全成功之后** 才会返回的。具体来说，当服务器收到了客户端发送的第三次握手的 `ACK` 报文，服务器端的这个连接状态就从 `SYN-RCVD` 变成了 `ESTABLISHED`。这时候，内核会把这个已经建立好的连接放到一个叫做“**全连接队列**”或者“**接受队列**” (`accept queue`) 里。

`accept` 函数的作用，其实就是应用程序从这个队列里取出一个已经建立好的连接，然后返回一个新的文件描述符（`socket`），后续的数据收发就用这个新的描述符。所以，如果 `accept` 能成功返回，就表示至少有一个 `TCP` 连接已经顺利完成了`三次握手`，处于 `ESTABLISHED` 状态，并且在等待应用程序来处理它了。如果队列是空的，那 `accept` 通常会阻塞，直到有新的连接建立完成并放入队列。

---

**(面试官):** 我们聊了序列号、确认号和一些标志位。TCP 头里还有哪些关键信息，它们是做什么用的？

**(面试者):** 嗯，除了我们聊的那些，TCP 头里还得有源端口和目标端口号，这样操作系统才知道把数据包交给哪个应用程序。还有一个很重要的字段是“窗口大小”，这个是用来做流量控制的，就是告诉对方：“我现在最多还能接收多少数据，你悠着点发”。还有一个校验和字段，用来检查数据在传输过程中有没有损坏。哦对了，有时候还有一些“选项”字段，比如在握手的时候双方可以协商一下最大报文段长度（MSS）之类的。

**(面试官):** 刚才我们说连接是双方维护的状态。那到底什么是“TCP 连接”？它由什么组成的？

**(面试者):** 它不像是一根实际的线缆。说白了，一个 TCP 连接其实就是通信双方（客户端和服务器）共同维护的一套“状态信息”。这套信息里包含了几个关键东西：首先是双方的 IP 地址和端口号，这四个值组合起来（也就是我们常说的四元组）能唯一地认出这个连接；然后是当前的序列号和确认号，用来追踪数据流，保证顺序和确认收到；还有就是刚才提到的窗口大小，用来控制流量。所谓的“建立连接”，本质上就是双方通过握手，就这些初始状态信息达成一致，并且在各自的系统里把这套状态建立起来的过程。

**(面试官):** 你提到了四元组。在一个繁忙的服务器上，同时可能有成百上千个连接，系统是怎么准确区分哪个数据包属于哪个连接的呢？

**(面试者):** 这就得靠那个四元组了：源 IP 地址、源端口号、目标 IP 地址、目标端口号。这四个信息组合在一起，就能在整个网络中唯一地标识一个 TCP 连接。就算同一个客户端反复连接服务器上同一个端口，它每次发起连接时，操作系统通常会给它分配一个不同的、临时的源端口号，这样一来，每个连接的四元组还是独一无二的。内核收到数据包后，就会根据包头里的这四个值，去查找对应的连接状态记录，然后把数据交给正确的处理程序。

**(面试官):** 明白了。我们一直在说 TCP 可靠。那和它经常一起被提起的 UDP 相比呢？它们的核心区别在哪？什么时候该用哪个？

**(面试者):** UDP 跟 TCP 的设计哲学可以说正好相反。UDP 是无连接的，比较“随缘”，只管把数据报发出去，但不保证对方一定能收到，也不保证顺序，更不会检查有没有重复。你可以把它想象成寄平信，发出去了就完事了，丢没丢、先到后到都不管。而 TCP 就像是寄挂号信，得先联系好，送到了要收条，丢了还得重寄。UDP 的好处是啥呢？因为它省掉了握手、确认、重传这些复杂的机制，所以它的开销非常小，速度也快得多。所以，如果你对实时性要求很高，并且能容忍偶尔丢几个包，比如在线看视频、听音乐，或者像 DNS 查询这种，一次请求很小，就算丢了上层应用再问一次也很快的场景，就适合用 UDP。而像文件传输、浏览网页这种，要求一个字节都不能错、不能丢的应用，那就必须用 TCP 来保证可靠性了。

**(面试官):** 那有没有可能，在同一台服务器上，让一个 TCP 服务和 一个 UDP 服务监听同一个端口号呢？比如 TCP 的 80 端口和 UDP 的 80 端口？

**(面试者):** 完全可以。你可以想象成同一个门牌号下，有给 TCP 协议的信箱，也有给 UDP 协议的信箱，它们是分开的。当一个 IP 数据包到达服务器时，IP 头里会有一个“协议”字段，明确标明了里面装载的数据是 TCP（协议号是 6）还是 UDP（协议号是 17）。操作系统会先看这个协议号，然后把包交给对应的协议栈（TCP 栈或 UDP 栈）去处理。TCP 和 UDP 各自维护着一套独立的端口号使用情况。所以，监听 TCP 80 端口和监听 UDP 80 端口是两个完全独立的操作，它们可以同时存在，互不干扰。

**(面试官):** 如果你需要在一台 Linux 服务器上查看当前 TCP 连接的状态，比如哪些连接是 ESTABLISHED，哪些是 TIME_WAIT，你会用什么命令？

**(面试者):** 比较经典的是 netstat 命令，通常会带上一些参数，比如 netstat -napt，-n 表示显示数字形式的 IP 和端口，不去做域名解析，-a 表示显示所有监听和非监听的连接，-p 显示哪个进程在使用这个连接，-t 就是只看 TCP 的。还有一个更现代、据说效率更高的命令是 ss，用法也类似，比如 ss -napt，它在连接数非常多的时候通常比 netstat 快。用这些命令就能看到每个连接的状态（比如 ESTABLISHED, SYN_SENT, TIME_WAIT, CLOSE_WAIT 等等），还有本地和远端的地址端口信息。

**(面试官):** 我们之前提到三次握手时要同步初始序列号 ISN。这个 ISN 是怎么产生的？是纯粹的随机数吗？

**(面试者):** 它需要做到不可预测，主要是为了安全。如果攻击者能猜到序列号，那就有可能伪造数据包来劫持连接。同时，它也不能在短时间内重复，不然可能会跟之前连接的延迟数据包搞混。所以，它不是简单的随机数。现在的系统通常是结合两种方式来生成 ISN：一个是基于一个高精度时钟，让 ISN 大体上是随时间递增的；另一个是会加入一些与连接本身相关的信息（比如源/目的 IP 和端口，也就是四元组）进行某种哈希计算（比如用一个秘密密钥做个摘要）。这样生成的 ISN 既随时间变化，又与具体连接有关，使得它很难被外部预测，同时也保证了短时间内不容易出现重复。

**(面试官):** 还有一个效率问题。IP 层本身就能对大数据包进行分片。那为什么 TCP 层还要多此一举，在握手时协商一个最大报文段长度（MSS）呢？

**(面试者):** 这主要是为了提高重传效率。你想，如果 TCP 直接扔给 IP 层一个非常大的数据块，比如 IP 层把它分成了 10 个小分片。万一在传输过程中，这 10 个小分片里有任何一个丢了，会发生什么？因为负责保证可靠性和重传的是 TCP 层，而 TCP 是按它自己发送的“段”（Segment）来管理的，它可能不知道底下 IP 层具体哪个小分片丢了，最坏的情况是它必须重传整个最初那个大 TCP 段。这就非常浪费带宽了。所以，TCP 在三次握手时会主动协商一个 MSS 值（通常基于路径 MTU 减去 IP 头和 TCP 头的开销），目标是让 TCP 自己生成的每个数据段都能刚好（或小于）放到一个 IP 包里，而不需要 IP 层再进行分片。这样，万一传输中丢了一个包，TCP 只需要重传这一个 MSS 大小的数据段就行了，效率高得多。

**(面试官):** 我们再细化一下握手和挥手过程中丢包的情况。比如三次握手时，客户端发的第一个 SYN 包丢了，会怎么样？

**(面试者):** 如果客户端发的 SYN 丢了，那服务器自然收不到，也就不会回 SYN-ACK。客户端傻等一会儿，就会超时。超时之后，客户端会重新发送一个完全一样的 SYN 包（序列号也一样）。它通常会尝试几次重传，而且每次重传的间隔可能会逐渐变长（比如等 1 秒，再等 2 秒，再等 4 秒这样）。如果重传了好几次（这个次数可以由系统参数控制），还是没收到服务器的回应，客户端最终就会放弃这次连接尝试，报连接失败。

**(面试官):** 那如果是服务器回给客户端的 SYN-ACK 包丢了呢？

**(面试者):** 这个情况稍微复杂点，因为两边可能都在等对方。客户端那边还在等 SYN-ACK，等不到就会超时重传它的 SYN。服务器这边呢，它发了 SYN-ACK 之后，就在等客户端的最后那个 ACK。如果 SYN-ACK 丢了，服务器自然也等不到客户端的 ACK，服务器自己也会超时，然后它会重传它的 SYN-ACK 包。所以这时候，可能客户端在重传 SYN，服务器在重传 SYN-ACK，直到某一方的数据包成功到达对端，握手才能继续下去。

**(面试官):** 那握手的最后一步，客户端发的 ACK 丢了呢？

**(面试者):** 这个情况又有点不一样。客户端发出最后的 ACK 后，它自己就认为连接建立成功了，状态变成 ESTABLISHED。但服务器那边，它还在 SYN-RCVD 状态，苦苦等待这个 ACK。如果 ACK 丢了，服务器等超时后，它不会认为连接失败，而是会觉得可能是自己之前发的 SYN-ACK 对方没收到，所以它会重新发送 SYN-ACK。当这个重发的 SYN-ACK 到达客户端时，客户端发现自己明明已经 ESTABLISHED 了，却又收到一个 SYN-ACK，它就知道：“哦，看来我上次发的 ACK 对方没收到”。于是，客户端会再次发送一个 ACK 给服务器。这样，服务器最终总能收到 ACK，然后也进入 ESTABLISHED 状态。所以连接还是能建立成功，只是可能稍微延迟了一点。

**(面试官):** 类似地，四次挥手过程中，如果客户端先发的 FIN 丢了呢？

**(面试者):** 客户端发出 FIN 后，进入 FIN_WAIT_1 状态，等待服务器的 ACK。如果 FIN 丢了，客户端自然收不到 ACK。等超时后，客户端会重新发送 FIN 包，同样会尝试几次。如果一直失败，最后客户端可能会直接放弃，强制关闭连接（进入 CLOSED 状态）。

**(面试官):** 如果是服务器对第一个 FIN 的 ACK 丢了呢？

**(面试者):** 这时，客户端还卡在 FIN_WAIT_1 状态，因为它没收到 ACK，所以会超时重传 FIN。服务器其实已经收到了第一个 FIN，并且进入了 CLOSE_WAIT 状态，也发送了 ACK。如果这个 ACK 丢了，服务器就待在 CLOSE_WAIT 不动（它可能还在处理自己要发的数据）。当它收到客户端重传过来的 FIN 时，它就知道：“哦，看来我上次发的 ACK 对方没收到”，于是它会再次发送那个 ACK。直到客户端收到这个 ACK，才能进入 FIN_WAIT_2 状态。

**(面试官):** 如果是服务器发送的 FIN（挥手的第三步）丢了呢？

**(面试者):** 服务器发完自己的数据，发送 FIN 后，进入 LAST_ACK 状态，等待客户端最后的 ACK。如果这个 FIN 在路上丢了，客户端（很可能在 FIN_WAIT_2 状态）就一直收不到，也就不会发送最后的 ACK。服务器在 LAST_ACK 状态等超时后，会重新发送它的 FIN 包。直到客户端收到了这个 FIN，它才会发送最后的 ACK，并进入 TIME_WAIT 状态。

**(面试官):** TIME_WAIT 状态很重要我们聊过了。但如果服务器上出现特别多的 TIME_WAIT 连接，会有什么实际危害吗？

**(面试者):** 虽然它本身是为了可靠性，但数量太多确实不好。每一个 TIME_WAIT 状态的连接，都还占用着一些系统资源，比如内存（用来保存连接状态信息），还有一个文件描述符。更关键的是，尤其对于需要频繁主动发起连接的一方（比如客户端，或者进行大量后端调用的服务器），每一个 TIME_WAIT 连接都会占用一个本地端口号。端口号是有限的（比如 Linux 上默认可能就几万个可用端口），如果短时间内产生大量的 TIME_WAIT 连接，并且都绑定在同一个 IP 地址上，去连接同一个目标 IP 和端口，就可能把可用的源端口号耗尽。这时候，程序再想发起新的连接，就会失败，可能会报“地址已被使用”（Address already in use）之类的错误。对于只负责监听、被动接受连接的服务器来说，端口耗尽问题通常不严重，主要是系统资源的消耗。

**(面试官):** 那有没有什么办法可以优化或者缓解 TIME_WAIT 过多的问题？

**(面试者):** 有一些内核参数可以调整。比如 tcp_tw_reuse 这个参数，如果开启，内核允许在安全的情况下（比如确认时间戳足够新，旧连接的包应该已经消失了）为一个新的出站连接 复用一个处于 TIME_WAIT 状态的 socket。这对于需要大量发起连接的客户端场景比较有用。以前还有一个叫 tcp_tw_recycle 的参数，但它在有 NAT 的网络环境下容易出问题，现在基本被废弃了，不推荐使用，reuse 是相对更安全的选择。还有一种比较“暴力”的方法，是通过设置 socket 选项（比如 SO_LINGER 设置为 0），让 close() 调用直接发送 RST 包而不是标准的 FIN，这样就跳过了四次挥手和 TIME_WAIT。但这样做很危险，可能会导致数据丢失，因为对方可能还没收到所有数据，连接就被强制重置了，所以一般强烈不推荐。通常还是优先考虑调整 tcp_tw_reuse，或者从应用架构层面看看能不能减少主动关闭连接的次数，比如使用长连接。

**(面试官):** 如果你发现一台服务器上出现了大量的 TIME_WAIT 状态，这通常说明了连接关闭模式是怎样的？

**(面试者):** 这直接说明，在这台服务器上，是服务器端在主动发起连接关闭（也就是发送第一个 FIN 包）的操作。记住，只有主动关闭方才会进入 TIME_WAIT 状态。这种情况很常见，比如 HTTP 服务器配置了短连接（Keep-Alive 关闭），那服务器每次回完响应就主动关连接。或者即使开了长连接，如果客户端一直不发新请求，超过了服务器设定的 keepalive_timeout，服务器为了回收资源也可能会主动关闭这个空闲连接。还有，如果服务器对单个长连接处理的请求数量有限制，达到限制后也可能主动关闭。总之，服务器上 TIME_WAIT 多，意味着服务器是那个“先说再见”的角色。

**(面试官):** 那反过来，如果看到服务器上有很多连接卡在 CLOSE_WAIT 状态呢？这又说明什么问题？

**(面试者):** CLOSE_WAIT 状态就完全不一样了。连接进入 CLOSE_WAIT，意味着服务器已经收到了客户端发来的 FIN（表示客户端那边不准备再发数据了），并且服务器的内核也已经回复了 ACK 给客户端。现在，连接正停在这一步，等待服务器上的应用程序调用 close() 来关闭它自己这一端的连接。如果看到大量连接长时间停在 CLOSE_WAIT，几乎 100% 是服务器端的应用程序出了问题。很可能是应用程序的代码逻辑有缺陷：比如程序收到了对方关闭的信号（例如 read() 函数返回了 0 或 -1 表示 EOF），但是程序没有执行相应的 close() 操作。这可能是忘记写 close() 调用了，或者是程序卡在某个地方、死锁了，导致执行不到 close() 那一步，也可能是文件描述符没有被正确地加入到事件监听（像 epoll/select）中，导致程序根本没意识到这个连接该关了。总之，CLOSE_WAIT 堆积，锅基本都在应用层代码。

**(面试官):** 假设 TCP 连接已经建立好了，双方正在通信，但客户端那台机器突然死机或者断网了，服务器这边会怎么样？连接会一直傻等着吗？

**(面试者):** 如果客户端是突然“物理消失”，没有机会发送 FIN 包，那服务器这边确实不知道。这个连接在服务器看来还是 ESTABLISHED 状态，它会一直维持着，白白占用资源。为了处理这种情况，TCP 提供了一个可选的“保活”（Keepalive）机制。如果应用程序给这个 socket 开启了 Keepalive 选项，那么当连接长时间（比如默认 2 小时）没有任何数据传输时，服务器的内核就会自动开始向客户端发送一些“探测包”。如果连续发送好几个探测包（比如默认 9 次，每次间隔 75 秒），客户端都没有任何回应，内核就会认为这个连接已经死掉了，然后会自动断开这个连接，并通知服务器应用程序。不过，TCP 自带的这个 Keepalive 机制默认的探测时间和间隔都非常长，可能要两个多小时才能发现问题。所以，在实际应用中，更常见的做法是在应用层自己实现心跳（Heartbeat）机制。比如，服务器可以要求客户端每隔几十秒就发一个心跳包过来，如果服务器在规定时间内没收到心跳，就主动判断连接失效并关闭它。或者像 Web 服务器那样，有 keepalive_timeout 设置，一段时间没活动就主动关，这都比依赖内核的 TCP Keepalive 要快得多。

**(面试官):** 那如果是另一种情况，连接也建立好了，但是处理这个连接的服务器进程突然崩溃了呢？这个连接会怎么样？

**(面试者):** 这个情况操作系统内核会处理得很优雅。TCP 连接的状态信息主要是由内核来维护的，不完全依赖于用户进程。当一个服务器进程崩溃退出时，内核会进行资源回收，其中就包括这个进程打开的所有文件描述符，当然也包括它正在处理的那些网络连接的 socket。内核发现这个 socket 对应的进程没了，它就会接管过来，代表这个已经崩溃的进程，主动向连接的另一端（也就是客户端）发起 TCP 的关闭流程——内核会自动发送一个 FIN 包给客户端，启动标准的四次挥手过程。所以，对于客户端来说，它看到的现象就好像是服务器那边正常地关闭了连接，即使服务器应用程序实际上是异常崩溃了。连接不会一直悬挂着。